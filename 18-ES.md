## 概述

ES 官方文档，根据版本查询对应API，本文档不一定符合当前版本

```http
https://www.elastic.co/guide/en/elasticsearch/reference/index.html
```

优点：

* 横向可扩展，扩大集群容易。
* 分片机制提供更好的分布性。
* 高可用，提供复制机制。
* 使用简单。

全文搜索是指通过扫描文章中的每一个词，对每个词建立一个索引，指明该词在文章中出现的次数和位置。

倒排索引：对文档或者文档集合中的单词建立索引，用来储存这些单词在文档或者一组文档中出现的频度、位置。

术语：

* 索引词：一个能够被索引的精确词。
* 文本：一段普通的非结构化文字，通常文本会被分析成一个个索引词。
* 分析：将文本转换为索引词的过程，分析结果依赖于分词器。
* 集群：集群由一个或多个节点组成，对外提供服务。在所有节点中，一个集群由一个唯一的名称，默认为 ElasticSearch，不同的集群应当使用不同的名称，节点是通过集群名称加入集群的。
* 节点：一个逻辑上独立的服务，节点也有唯一的名字，在网络中集群通过名字来管理和通信，当网络中没有集群运行时，只要启动任何一个节点，就会生成一个只有一个节点的集群。
* 路由：储存文档时，会储存在唯一的主分片中，具体哪个分片时通过散列值进行选择，默认情况下，散列值由文档ID生成，如果文档存在一个父文档，则由父文档ID生成。
* 分片：单个Lucene(全文搜索引擎工具包)实例，文档储存在分片中，分片分配在集群的节点，当集群扩容或缩小时，ElasticSearch 将自动在节点间迁移分片，以使集群保持平衡。
* 主分片：储存文档时，系统会首先储存在主分片中，然后会复制到不同的副本，默认情况一个索引有5个分片，分片一旦建立，数量就不能修改。
* 副本分片：每个分片有零个或多个副本，副本是主分片的复制，有两个目的：

  1. 增加高可用性，允许水平分割扩展数据：当主分片失败时，可用从副本中选择一个作为主分片。
  2. 提高性能，允许分配和并行操作提高吞吐量：可以通过主分片或者副本分片进行查询。默认情况下，一个主分片配有一个副本，但副本的数量可以在后面动态地配置增加。副本分片必须部署在不同的节点，不能部署在和主分片相同的节点。

  默认情况下，每个索引分配5个分片和一个副本，意味着集群节点至少要有两个，将拥有5个主分片、5个副本分片。
* 索引：名词表示 具有相同结构的文档集合，对应关系型数据库的 Database，动词表示把文档储存到索引(名词)中，对应关系型数据库的 Insert 操作。
* 类型：在索引中可以定义一个或多个类型，类型是索引的逻辑分区，对应关系型数据库的 Table。
* 文档：储存在 ElasticSearch 中的一个JSON格式的字符串，代表一行数据，对应关系型数据库的 Row。每个储存在索引中的文档都有一个类型和一个ID，每个文档都是一个JSON对象，储存了零个或多个字段，原始的JSON文档被储存在 _source 的字段中，当搜索文档时默认返回的就是这个字段。
* 映射：映射像关系数据库中的表结构，每个类型都有一个映射。
* 字段：文档中包含零个或多个字段，对应关系型数据库的 Column。
* 来源字段：默认情况下，原文档储存在 _source 字段。
* 主键：如果存库时没有，会自动生成一个，文档的 index/type/id 必须唯一。

分布式特性：

* 将文档分区到不同的容器或者分片(shards)中，它们可以存在于一个或多个节点中。
* 将分片均匀的分配到各个节点，对索引和搜索做负载均衡。
* 冗余每一个分片，防止硬件故障造成的数据丢失。
* 将集群中任意一个节点上的请求路由到相应数据所在的节点。
* 无论是增加节点，还是移除节点，分片都可以做到无缝的扩展和迁移。

## 常用配置

```yaml
# 集群名称，节点通过该值加入集群
cluster.name
# 节点名称
node.name
# 机架名称
node.rack
# 索引储存位置
path.data
# 日志储存位置
path.logs
# 内存分配模式，为true时先获取配置大小内存，再加入集群，并且禁止内存和磁盘交换
bootstrap.mlockall
# 绑定ip地址，默认127.0.0.1
network.host
# http端口
http.port
```

### Http配置

![412.png](assets/412.png)

## 基本操作

### curl

格式：`curl <options> <url>`

options：

* -o：`-o <file-name>` , 将请求保存为文件。
* -O：用原文件名保存。
* -C -：断点继续下载。
* -i：打印响应头。
* -b：传递cookie。
* -H：指定header，例如`-H 'content-type: application/json'`
* -X：指定请求方法，例如`-X POST`
* -d：指定请求数据。
* -F：向服务器上传二进制文件，例如`-F 'file=@photo.png;type=image/png'`
* -k：跳过ssl检测。
* -L：跟随重定向。
* -s：不输出错误信息和进度信息。
* -S：只输出错误信息。
* -u：设置用户名密码。

### 索引

#### 创建索引

```bash
PUT http://127.0.0.1:9200/索引名称/
{
  "settings" : {
       "index" : {
         "number_of_shards" : 3,
         "number_of_replicas" : 2
       }
  }
}
# 或者简写为
{
   "settings": {
     "number_of_shards": 3,
     "number_of_replicas": 2
   }
}
```

#### 打开关闭索引

关闭的索引只能显示索引元数据信息，不能够进行读写操作。

```bash
POST 127.0.0.1:9200/索引名称/_close
```

#### 新增索引别名

一个别名可以关联多个索引

```bash
POST http://localhost:9200/_aliases
{
 "actions" : [
     { "remove" : { "index" : "test1", "alias" : "alias1" } },
     { "add" : { "index" : "test1", "alias" : "alias2" } }
 ] 
}
```

### 映射

映射分为两类：

* 动态映射：根据写入文档自动添加字段和字段，映射模式可通过 mappings.dynamic 字段 设置。
* 显示映射：需要手动精确定义，例如哪些字段为全文字段，哪些是数字、日期字段。

#### 动态映射

dynamic策略：


| 策略    | 备注                                                      |
| ------- | --------------------------------------------------------- |
| true    | 默认值，自动添加新字段，一旦创建会生成倒排索引，无法更改  |
| runtime | 自动添加新字段，但不会生成倒排索引，由读时再进行计算      |
| false   | 不会添加新字段，不能通过新字段查询，会存储在 _source 字段 |
| strict  | 发现新字段会抛出异常                                      |

* 日期映射

  默认日期格式为 `yyyy/MM/dd HH:mm:ss||yyyy/MM/dd||epoch_millis`

  ```shell
  #禁用日期自动检测，默认开启
  PUT 索引名称
  {
    "mappings": {
      "date_detection": false
    }
  }
  #自定义日期格式
  PUT 索引名称
  {
    "mappings": {
      "dynamic_date_formats": ["yyyy/MM/dd"]
    }
  }
  ```
* 数字映射

  ```shell
  #启用数值检测模式，默认不启用
  PUT 索引名称
  {
    "mappings": {
      "numeric_detection": true
    }
  }
  ```

##### 动态映射模板

当dynamic策略设置为true或runtime时，可按照模板进行索引，匹配字段的条件有两种类型：

* match_mapping_type: 对检测到的数据类型进行匹配。
* match、unmatch：通过模式匹配来匹配字段名称。

例如：

```shell
PUT 索引名称
{
    "mappings": {
        "dynamic_templates": [
            {
                "模板名称": {
                    "match_mapping_type": "检测到数据类型",
                    "mapping": {
                        "type": "索引的类型"
                    }
                }
            }
        ]
    }
}
```

1. 结构化搜索：对数据不需要分词，不进行全文搜索，仅对字段进行精确匹配。

```shell
# 默认会将字符串映射为text字段，这里将所有String类型字段映射为keyword，不进行分词。
PUT 索引名称
{
  "mappings": {
    "dynamic_templates": [
      {
        "模板名称": {
          "match_mapping_type": "string",
          "mapping": {
            "type": "keyword"
          }
        }
      }
    ]
  }
}
```

2. 仅全文搜素：对字符串字段全部进行分词索引，不进行聚合、排序、精确匹配。

```shell
PUT  索引名称
{
  "mappings": {
    "dynamic_templates": [
      {
        "模板名称": {
          "match_mapping_type": "string",
          "mapping": {
            "type": "text"
          }
        }
      }
    ]
  }
}
```

3. 不进行评分搜素,节约一部分空间。

```shell
PUT 索引库名称
{
  "mappings": {
    "dynamic_templates": [
      {
        "模板名称": {
          "match_mapping_type": "string",
          "mapping": {
            "type": "text",
            #关闭评分功能
            "norms": false
          }
        }
      }
    ]
  }
}
```

4. 数字字段禁用索引，因为数字字段通常进行聚合计算，并不会进行匹配，节约一部分空间。

```shell
PUT 索引库名称
{
  "mappings": {
    "dynamic_templates": [
      {
        "模板名称": {
          "match_mapping_type": "long",
          "mapping": {
            "type": "long",
            #禁止创建索引
            "index": false
          }
        }
      }
    ]
  }
}
```

5. 字段名模式匹配

```shell
PUT 索引名称
{
    "mappings": {
        "dynamic_templates": [
            {
                "模板名称": {
                    "match_mapping_type": "string",
                    "match": "long_*",
                    "unmatch": "*_text",
                    "mapping": {
                        "type": "long",
                        "enable": false
                    }
                }
            },
            {
                "模板名称": {
                    "match_pattern": "regex",
                    "match": "^profit_\\d+$",
                    "mapping": {
                        "type": "long",
                        "index": false,
                        "norms": false,
                        "doc_values": false
                    }
                }
            }
        ]
    }
}
```

6. 子对象路径匹配

```shell
PUT 索引库名称
{
  "mappings": {
    "dynamic_templates": [
      {
        "模板名称": {
          "path_match":   "*",
          "path_unmatch": "*.m",
          "mapping": {
            "type":       "text",
            "copy_to":    "full_name"
          }
        }
      }
    ]
  }
}
```

#### 字段类型

##### 常用类型

1. alias：可为其他字段定义别名, 可使用别名搜索，不可使用别名写入，例如：

    ```shell
    PUT 索引名称
    {
      "mappings": {
          "别名字段名称": {
            #别名类型
            "type": "alias",
            "path": "原字段名称" 
          }
        }
      }
    }
    ```

2. array：数组中所有值必须具有相同的字段类型。
3. binary：可储存二进制。
4. boolean：可存储布尔值，也可储存"true"、"false"的字符串，不可储存0、1。
5. date：时间格式的字符串。
6. object、nested：对象和嵌套对象，可通过 nested 关键字对 nested 字段类型进行搜索。
7. range：该类型细分为integer_range、float_range、long_range、double_range、date_range、ip_range，date_range不管什么格式都被解析为毫秒进行比较大小。
8. numbers：该类型细分为long、integer、short、byte、double、float、half_float、unsigned_long。

##### rank_feature

rank_feature 或 rank_features 字段的数值可以提高文档的相关性分数，他们作用于数字字段，默认值越大相关性越高，也可通过 positive_score_impact = false 使值越大相关性越低。

rank_feature使用以下函数计算分数：

* saturation：默认值，score区间(0,1)，打分公式为 `S / (S + pivot)` ，S是字段的值，pivod是score分界值，默认使用字段近似平均值，当S值大于pivot时，score>0.5 ，当S值小于pivot时，score<0.5 。
* Logarithm：score无边界，打分公式为 `log(scaling_factor + S)` ，S是字段的值，scaling_factor 是配置的缩放系数。
* Sigmoid：score区间(0,1)，saturation 的扩展，打分公式为 `S^exp / (S^exp + pivot^exp)` ，exp 为指数，建议取值区间 `[0.5,1]`。
* 字符串数据类型

- 全文本：可以用于分词，不用于排序而且很少聚合，用于基于文本的相关性搜索。
- 关键字：不参与分词，通常用于过滤，排序以及聚合。

字符串的参数：


| 参数                   | 备注                                                                                                                                                                                                                                    |
| ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| analyzer               | 分词器可以被用于可分词的字符串型字段。<br />默认为默认的索引分词器，或者标准分词器。                                                                                                                                                    |
| boost                  | 字段级索引加权。接受浮点型数字，默认值是 1.0。<br />doc_values 定义字段是否应该以列跨度的方式存储在磁盘上，<br />以便用于排序，聚合或者脚本？接受 true 或 false 参数。<br />对于不可分词字段，默认值是 true。可分词字段不支持这个参数。 |
| fielddate              | 决定字段是否可以使用内存字段值进行排序，聚合或者在脚本中使用。<br />接受 disabled 或者 paged_bytes（默认）参数。<br />没有分析过的字段会优先使用文档值。                                                                                |
| ignore_above           | 不要索引或执行任何长于这个值的字符串。默认为 0（禁用）。                                                                                                                                                                                |
| include_in_all         | 决定字段是否应该被包含在_all 字段中。接受 true 或 false 参数。<br />如果索引被设置为 no 或者父对象字段设置 include_in_all 为 false，<br />参数默认值为 false；其它情况下，默认值为 true。                                               |
| index                  | 决定字段是否可以被用户搜索。<br />接受参数 analyzed（默认，视为全文字段），<br />not_analyzed（作为关键字字段）以及 no。                                                                                                                |
| index_options          | 定义存储在索引中，用于搜索和突出用途的信息。                                                                                                                                                                                            |
| norms                  | 计算查询得分的时候是否应该考虑字段长度。<br />默认依赖于索引设置： analyzed <br />字段默认{ "enabled": true, "loading": "lazy" }。<br />not_analyzed 字段默认{ "enabled": false }。                                                     |
| null_value             | 接受一个字符串值替换所有 null 值。默认为 null，意味着字段被作为缺失字段。<br />如果字段是可分词（analyzed）的，null_value 也会被分词。                                                                                                  |
| position_increment_gap | 定义字符串数组中应该插入的虚拟索引词的数量。<br />默认值为100，以一个较合理的值来阻止短语查询在跨字段匹配索引词的时候溢出。                                                                                                             |
| store                  | 决定字段值是否应该被存储以及从_source 字段分别获取。<br />接受参数 true 或 false（默认）。                                                                                                                                              |
| search_analyzer        | 指定搜索时用在可分词字段上的分词器。                                                                                                                                                                                                    |
| search_quote_analyzer  | 指定搜索短语时使用的分词器。                                                                                                                                                                                                            |
| similarity             | 指定使用的相似度评分算法，默认为 TF/IDF。                                                                                                                                                                                               |
| term_vector            | 定义一个可分词字段是否应该存储索引词向量。默认为 no。                                                                                                                                                                                   |

* 数字型数据类型


| 类型    | 备注                                                     |
| ------- | -------------------------------------------------------- |
| long    | 一个有符号的 64 位整数，最小值为 -2^63 ，最大值为 2^63-1 |
| integer | 一个有符号的 32 位整数，最小值为-2^31 ，最大值为 2^31-1  |
| short   | 一个有符号的 16 位整数，最小值为-32768，最大值为 32767。 |
| byte    | 一个有符号的 8 位整数，最小值为-128，最大值为 127。      |
| double  | 64 位双精度浮点数。                                      |
| float   | 32 位单精度浮点数。                                      |

数字的参数：


| 参数             | 备注                                                                                                                                                                                      |
| ---------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| coerce           | 试着将字符串型数据转换为整数型数字数据。                                                                                                                                                  |
| boost            | 字段级索引加权，接受浮点型数字参数，默认为 1.0。                                                                                                                                          |
| doc_values       | 定义字段是否应该以列跨度的方式存储在磁盘上，<br />以便用于排序，聚合或者脚本？接受 true（默认）或 false 参数。                                                                            |
| ignore_malformed | 如果是 true，畸形的数字会被忽略。<br />如果是 false（默认），畸形数字会抛出异常并丢弃整个文档。                                                                                           |
| include_in_all   | 决定字段是否应该被包含在_all 字段中。接受 true 或 false 参数。<br />如果索引被设置为 no 或者父对象字段设置 include_in_all 为 false，<br />参数默认值为 false；其他情况下，默认值为 true。 |
| index            | 决定字段是否可以被用户搜索。<br />接受参数 not_analyzed（默认）以及 no。 <br />null_value 接受与字段同类型的数字型值来代替 null 值。<br />默认是 null，意味着字段被作为缺失字段。         |
| precision_step   | 控制索引的额外索引词的数量来使范围查询更快速。<br />默认值取决于数字类型。                                                                                                                |
| store            | 决定字段值是否应该被存储以及从_source 字段分别获取。<br />接受参数 true或 false（默认）。                                                                                                 |

* 时间类型

默认值为 `strict_date_optional_time||epoch_millis`，`||` 表示多时间格式，会依次尝试。


| 参数           | 备注                                                                                                                                                                                            |
| -------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| boost          | 字段级索引加权，接受浮点型数字参数，默认为 1.0。                                                                                                                                                |
| doc_values     | 定义字段是否应该以列跨度的方式存储在磁盘上，<br />以便用于排序，聚合或者脚本？接受 true（默认）或 false 参数。                                                                                  |
| format         | 可解析的日期格式。<br />ignore_malformed 如果是 true，畸形的日期会被忽略。<br />如果是 false（默认），畸形日期会抛出异常并丢弃整个文档。                                                        |
| include_in_all | 决定字段是否应该被包含在_all 字段中。<br />接受 true 或 false 参数。<br />如果索引被设置为 no 或者父对象字段设置 include_in_all 为 false，<br />参数默认值为 false；其他情况下，默认值为 true。 |
| index          | 决定字段是否可以被用户搜索。<br />接受参数 not_analyzed（默认）以及 no。 <br />null_value 接受日期型值来代替 null 值。<br />默认是 null，意味着字段被作为缺失字段。                             |
| precision_step | 控制索引的额外索引词的数量来使范围查询更快速。默认值为 16。                                                                                                                                     |
| store          | 决定字段值是否应该被存储以及从_source 字段分别获取。<br />接受参数 true 或 false（默认）。                                                                                                      |

* 布尔类型

布尔型字段接受 true 或 false 值，也可以接受代表真或假的字符串和数字。
假值 false，“false”，“off”，“no”，“0”，“”（空字符串），0，0.0
真值 其他任何非假的值


| 参数       | 备注                                                                                                                                                                |
| ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| doc_values | 定义字段是否应该以列跨度的方式存储在磁盘上，<br />以便用于排序，聚合或者脚本？接受 true（默认）或 false 参数。                                                      |
| index      | 决定字段是否可以被用户搜索。<br />接受参数 not_analyzed（默认）以及 no。<br /> null_value 接受布尔型值来代替 null 值。<br />默认是 null，意味着字段被作为缺失字段。 |
| store      | 决定字段值是否应该被存储以及从_source字段分别获取。<br />接受参数true或false（默认）。                                                                              |

* 二进制数据类型

二进制数据类型接受 Base64 编码字符串的二进制值。字段不以默认方式存储而且不能被搜索。


| 参数       | 备注                                                                                                           |
| ---------- | -------------------------------------------------------------------------------------------------------------- |
| doc_values | 定义字段是否应该以列跨度的方式存储在磁盘上，<br />以便用于排序，聚合或者脚本？接受 true（默认）或 false 参数。 |
| store      | 决定字段值是否应该被存储以及从_source字段分别获取。<br />接受参数true或false（默认）。                         |

* 元字段


| 字段名        | 备注                                            |
| ------------- | ----------------------------------------------- |
| _index        | 文档所属的索引。                                |
| _uid          | 包含_type 和_id 的混合字段。                    |
| _type         | 文档的映射类型。                                |
| _id           | 文档的 ID                                       |
| _source       | 作为文档内容的原始 JSON。                       |
| _size _source | 元字段占用的字节数，通过 mapper-size 插件提供。 |
| _all          | 索引所有字段的值                                |
| _field_names  | 文档中所有包含非空值的字段。                    |
| _timestamp    | 关联文章的时间戳，可以手动指定或者自动生成。    |
| _ttl          | 定义文档被自动删除之前的存活时间。              |
| _parent       | 用于在映射类型之间创建父子关系。                |
| _routing      | 一个自定义的路由值，路由文档到一个特定的分片。  |
| _meta         | 应用特定的元字段。                              |

### 文档

#### 索引文档

```bash
# 指定 ID
PUT /<index>/<type>/<id>
{
  文档内容
}

# 自动生成ID，注意是POST
POST /<index>/<type>
{
  文档内容
}
```

添加新文档，确保不是更新：

```bash
PUT /<index>/<type>/<id>?op_type=create
{
  文档内容
}
# 或
PUT /<index>/<type>/<id>/_create
{
  文档内容
}
```

#### 更新文档

和索引文档同一个操作

```bash
# 更新成功后，_version 会加1，还会有 "created":false 属性
# 在内部是将旧文档标记删除，并且不能访问，然后添加新文档，文档时不可变的，不能被更改只能被替换
PUT /<index>/<type>/<id>
{
  文档内容
}
# 更新文档时指定版本，版本匹配才能修改
PUT /<index>/<type>/<id>?version=<num>
{
  文档内容
}
# 指定外部版本，比如数据库中版本，这个版本不需要完全匹配，只要后来版本大于之前版本就可以修改
PUT /<index>/<type>/<id>?version=<num>&version_type=external
{
  文档内容
}
```

#### 局部更新文档

更新请求接受一个局部文档参数doc，它会合并到现有文档中——存在的字段被覆盖，新字段被添加。

```bash
# 注意是POST
POST /<index>/<type>/<id>/_update
{
  "doc":{
     文档内容
   }
}
```

#### 检索文档

```bash
# 查询所有数据
POST /<index>/<type>/_search
# pretty参数，美化输出 响应内容包括 "found":true,如果找不到，则包括 "found":false,不管找到与否，状态码都是200
GET /<index>/<type>/<id>?pretty
# 输出部分文档
GET /<index>/<type>/<id>?_source=<field1>,<field2>
# 只请求_source
GET /<index>/<type>/<id>/_source
# 检测文档是否存在
curl -i -XHEAD http://localhost:9200/<index>/<type>/<id>
```

使用groovy脚本更新：

```bash
# ctx._source 表示 _source 的变量
# ctx.op 表示操作的变量，比如 delete 表示删除，none 表示什么都不做
POST /<index>/<type>/<id>/_update?retry_on_conflict=5  # 表示版本冲突时重试的次数
{
  "script":"groovy脚本",
  "params": {
     定义脚本变量
  },
  "upsert"：{
     如果不存在则新增
  }
}
```

检索多个文档：

```bash
GET /_mget
{
  "docs": [
     {
        文档1 条件
     },
     {
        文档2 条件
     },
   ] 
}
或者
GET /<index>/<type>/_mget
{
  "docs": [
     {
        文档1 条件
     },
     {
        文档2 条件
     },
   ] 
}
```

#### 删除文档

```bash
DELETE /<index>/<type>/<id>
```

#### 批量操作bulk

```bash
# 不是原子操作，各操作互不干扰
# 批量请求会先加载到请求节点的内存，所以请求越大，给其他请求可用内存越小。
POST /_bulk
{"create":{ 元数据 }}   # 当文档不存在时创建
{ 前一行操作的文档数据 }
{"update":{ 元数据 }}   # 局部更新文档
{ 前一行操作的文档数据 }
{"delete":{ 元数据 }}   # 删除一个文档
{ 前一行操作的文档数据 }
{"index":{ 元数据 }}   # 索引文档 或 更新文档
{ 前一行操作的文档数据 }
或
POST /<index>/<type>/_bulk
{"create":{ 元数据 }}   # 当文档不存在时创建
{ 前一行操作的文档数据 }
{"update":{ 元数据 }}   # 局部更新文档
{ 前一行操作的文档数据 }
{"delete":{ 元数据 }}   # 删除一个文档
{ 前一行操作的文档数据 }
{"index":{ 元数据 }}   # 索引文档 或 更新文档
{ 前一行操作的文档数据 }
```

由于批量操作中文档可能属于不同的主分片，意味着需要转发到对应的分片，为了减少解析数据，使用换行符识别数据行，减少内存消耗。

### API约定

* 多索引参数：

  1. `test1,test2,test3` : 表示同时搜索 test1，test2，test3 三个索引中的数据。
  2. `_all` : 内部关键字，表示全部索引。
  3. `+test*,-test3` : 表示查询所有 test 开头的索引，排除 test3。
  4. `ignore_unavailable` : 当索引不存在或者关闭的时候，是否忽略这些索引，不会抛出error。
  5. `allow_no_indices` : 当使用通配符查询所有索引的时候，当有索引不存在的时候是否返回查询失败。
  6. `expand_wildcards` : open 表示只支持开启状态的索引，close 表示只支持关闭状态的索引，none 表示 不可用，all 表示同时支持 open 和 close 索引。
  7. `<static_name{date_math_expr{date_format|time_zone}}>` : 按日期筛选

     ```bash
     # static_name ：索引的名称
     # date_math_expr：动态日期计算表达式
     # date_format ：日期格式
     # time_zone：时区，默认为 UTC
     curl -XGET 'localhost:9200/<logstash-{now%2Fd-2d}>/_search' {
       "query" : {
           ...
       }
     }
     # | 被转义为 %2F
     ```
* 通用参数

  1. `?pretty=true`,`?format=yaml` : 返回格式化后的JSON数据或者YAML数据。
  2. `+1h` 增加一个小时 `-1d` 减少一个小时 `/d` 上一个小时，所支持的时间单位为：y（年）、M（月）、w（周）、d（日）、h（小时）、m（分钟）、
     s（秒），表达式设定的日期为 now 或者日期字符串加||。

     ```bash
     # 当前时间加一小时，以毫秒为单位。
     now+1h
     # 当前时间加一小时和一分钟，以毫秒为单位。
     now+1h+1m
     # 当前时间加一小时，四舍五入到最近的一天。
     now+1h/d
     # 2015-01-01 加一个月，向下舍入到最接近的一天。
     2015-01-01||+1M/d
     ```
  3. 所有的返回值可以通过 filter_path 来减少返回值的内容，多个值可以用逗号分开，也可以使用 `*`(匹配任何部分字段的名称) 和 `**`(匹配不确定名称的字段) 通配符。

     ```bash
      curl -XGET 'localhost:9200/_nodes/stats?filter_path=nodes.*.ho*'
      {
        "nodes" : {
           "lvJHed8uQQu4brS-SXKsNA" : {
              "host" : "portable"
           }
        }
      }
     ```

### 分词器

* 测试分析器

```bash
GET localhost:9200/_analyze
{
   "analyzer" : "standard",
   "text" : "this is a test"
}
```

索引分析的过程：

1. 字符过滤器：将字符串过滤，例如去除HTML标记 或者转换 "&" 为 "and"。
2. 分词器：进行分词。
3. 标记过滤器：每个词都通过标记过滤器处理，可以修改词、去掉词、增加词等。

测试分词器：

```bash
GET localhost:9200/_analyze
{
  "analyzer" : "standard",
  "text" : "this is a test"
}
```

自定义分词器：

GET localhost:9200/_analyze
{
"tokenizer" : "keyword",
"token_filters" : ["lowercase"],
// 分词结果会去掉html标签
"char_filters" : ["html_strip"],
"text" : "this is a <b>test</b>"
}

每个查询、每个字段或每个索引都可以指定分析器。在创建索引时查找分析器的顺序：

* 在字段映射中定义的分析器。
* 在索引设置中名为 default 的分析器。
* 标准分析器。
* 在查询时，有更多的层次。
* 在全文查询中定义的分析器。
* 在字段映射中定义的搜索分析器。
* 在字段映射中定义的分析器。
* 在索引设置中名为 default_search 的分析器。
* 在索引设置中名为 default 的分析器。
* 标准分析器。

### 搜索

#### 常用搜索

```bash
#根据文档 _id查询文档详情
GET 索引库名称/_doc/文档_id
#查询指定索引库中的所有的文档信息
GET 索引库名称/_doc/_search
#查询当前集群中所有的索引库信息
GET /_cat/indices
#查询当前集群中所有的别名索引信息
GET /_cat/aliases
```

#### 空搜索

`GET /_search`返回集群索引中的所有文档。

返回结果：

* hits：返回匹配到的前10条数据。
* took：请求花费毫秒。
* shards：返回查询的分片数total，查询成功的分片数successful，查询失败的分片数failed。

`GET /_search?timeout=10ms`返回在请求超时前收集到的结果，后台并不会停止查询，只是不再返回结果。

#### 多索引、多类别

![1008.png](./assets/1008.png)

#### 分页

![1009.png](./assets/1009.png)

#### 简易搜索

`GET /_all/tweet1/_search?q=tweet:elasticsearch`：搜索类型为 tweet1 并在 tweet 字段包含 elasticsearch 字符的文档，可添加+前缀表示条件必须满足，-前缀表示必须不满足，例如 `+name:john +tweet:mary`

`GET /_search?q=mary`：搜索包含 mary 的所有结果。

#### 查询映射

`GET /gb/_mapping/tweet`

#### 结构化查询

##### 查询

match\_all 可以查询到所有文档，是没有查询条件下的默认语句。

```json
{
     "match_all": {}
}
```

match 查询是一个标准查询，不管你需要全文本查询还是精确查询基本上都要用到它。

使用 match 查询一个全文本字段，它会在真正查询之前用分析器先分析 match 一下查询字符：

```json
{
    "match": {
        "tweet": "About Search"
    }
}
```

使用 match 指定了一个确切值，在遇到数字，日期，布尔值或者 not\_analyzed 的字符串时，它将为你搜索你给定的值：

```json
{ "match": { "age": 26 }}
{ "match": { "date": "2014-09-01" }}
{ "match": { "public": true }}
{ "match": { "tag": "full_text" }}
```

bool 查询 合并多个查询子句，要计算每一个查询子句的 \_score （相关性分值）。

must :: 查询指定文档一定要被包含。

must\_not :: 查询指定文档一定不要被包含。

should :: 查询指定文档，有则可以为文档相关性加分。

```json
{
    "bool": {
      "must": { "match": { "title": "how to make millions" }},
      "must_not": { "match": { "tag": "spam" }},
      "should": [
          { "match": { "tag": "starred" }},
          { "range": { "date": { "gte": "2014-01-01" }}}
      ]
    }
}
```

##### 过滤

term 主要用于精确匹配哪些值。

```json
{ "term": { "age": 26 }}
{ "term": { "date": "2014-09-01" }}
{ "term": { "public": true }}
{ "term": { "tag": "full_text" }}
```

terms 允许指定多个匹配条件。 如果某个字段指定了多个值，文档则需要一起去做匹配。

```json
{
    "terms": {
        "tag": [ "search", "full_text", "nosql" ]
    }
}
```

range 过滤允许我们按照指定范围查找。

```json
{
    "range": {
        "age": {
            "gte": 20,
            "lt": 30
        }
    }
}
```

exists 和 missing 过滤可以用于查找文档中是否包含指定字段或没有某个字段。

```json
{
    "exists": {
        "field": "title"
    }
}
```

bool 过滤可以用来合并多个过滤条件查询结果的布尔逻辑，它包含一下操作符：

must :: 多个查询条件的完全匹配,相当于 and 。

must\_not :: 多个查询条件的相反匹配，相当于 not 。

should :: 至少有一个查询条件匹配, 相当于 or 。

```json
{
    "bool": {
        "must": { "match": { "tweet": "elasticsearch" }},
        "must_not": { "match": { "name": "mary" }},
        "should": { "match": { "tweet": "full text" }}
    }
}
```

##### 带过滤的查询语句

```bash
GET /_search
{
  "query": {
    "filtered": {
      "query": { "match": { "email": "business opportunity" }},
      "filter": { "term": { "folder": "inbox" }}
    }
  }
}
```

##### 排序

```bash
GET /_search
{
  "query" : {
    "filtered" : {
        "query": { "match": { "tweet": "manage text search" }},
        "filter" : { "term" : { "user_id" : 2 }}
    }
  },
  "sort": [
      { "date": { "order": "desc" }},
      { "_score": { "order": "desc" }}
  ]
}
```

## 集群

### 操作

#### 状态信息

```bash
#查询当前集群的颜色信息
GET /_cat/health

#查询当前集群的健康状态 
# status：集群状态，green（主分片和副本分片都可用），yellow（主分片可用，副本分片部分可用），red（部分主分片不可用）。
# number_of_nodes/number_of_data_nodes：集群节点数和数据节点数。
# active_primary_shards：集群中所有活跃的主分片数。
# active_shards：集群中活跃的分片数。
# relocating_shards：正在从一个节点迁往其他节点的分片的数量。通常为 0，集群变化时会变化，例如添加了一个新节点，或者下线了一个节点。
# initializing_shards：正在初始化的分片。
# unassigned_shards：未分配的分片，通常为0，当某个节点的副本分片丢失时，该值会变化，如果你的集群是 red 状态，也会存在未分配分片。
GET _cluster/health?pretty=true
GET _cluster/health?level=indices
GET _cluster/health?level=shards

#查询当前集群的运行状态信息
# 返回索引、文档、储存、节点、cpu、文件系统的统计信息
GET _cluster/stats?pretty

#查询当前集群中所有节点的监控信息，包含线程、jvm、索引查询指标等。
GET _nodes/stats?pretty
#查询当前集群中所有索引的监控信息
GET _stats?pretty

#查询当前集群中主节点的IP等信息
GET /_cat/master
#查询当前集群中所有的节点的IP等信息
GET /_cat/nodes
#查询当前集群中索引分片的信息
GET /_cat/shards
```

#### 升级节点

* 关闭分片分配：当关闭一个节点的时候，会立即试图复制这个节点的数据到集群的其他节点，将导致大量的 IO 请求，可以在关闭该节点时设置参数来避免。

  ```bash
  PUT /_cluster/settings
  {
    "transient": {
       "cluster.routing.allocation.enable": "none"
    }
  }
  ```
* 重新分配

  ```bash
  PUT /_cluster/settings
  {
    "transient": {
      "cluster.routing.allocation.enable": "all"
    }
  }
  ```
* 刷新数据到Lucene持久化保存

  ```bash
  POST /_flush
  ```
* 查看集群状态，当节点加入集群后，它首先恢复存储在本地的主分片数据，当每个节点都恢复完成后，集群的状态将会变成黄色，表示所有主分片已经被找到，但是并不是所有的副本分片都恢复。

  ```bash
  GET _cat/health
  ```

升级节点：

1. 关闭分片分配。
2. 刷新数据到Lucene持久化保存。
3. 升级节点。
4. 重新分配。

#### 备份与恢复

1. 创建快照仓库

```http
curl -XPOST http://192.168.0.1:9200/_snapshot/仓库名称/
{
    "type": "fs",
    "settings": {
        "location": "/data/backups/elasticsearch",
        "max_snapshot_bytes_per_sec" : "50mb",
        "max_restore_bytes_per_sec" : "50mb",
        "compress" : true
    }
}
```

`max_snapshot_bytes_per_sec`：制作快照的速度，默认20mb/s。

`max_restore_bytes_per_sec`：快照恢复的速度，默认20mb/s

2. 创建/删除备份

```http
curl -XPUT http://192.168.0.1:9200/_snapshot/仓库名称/快照名称
```

该操作是后台执行，默认备份所有索引，如果想等到备份完成，可以加上参数 wait_for_completion=true

```http
curl -XDELETE http://192.168.0.1:9200/_snapshot/仓库名称/快照名称
```

3. 查看备份

```http
curl -XGET http://192.168.0.1:9200/_snapshot/仓库名称/快照名称/_status
```

状态类型：

* INITIALIZING ：集群状态检查，检查当前集群是否可以做快照，通常这个过程会非常快
* STARTED ：正在转移数据到仓库
* FINALIZING ：数据转移完成，正在转移元信息
* DONE ：完成
* FAILED ：备份失败

4. 恢复

```http
curl -XPOST http://192.168.0.1:9200/_snapshot/仓库名称/快照名称/_restore 
{
    "indices": "index_1",
    "rename_pattern": "index_(.+)",
    "rename_replacement": "restored_index_$1"
}
```

其中：

* indices 表示只恢复 index_1。
* rename_pattern 表示对 index_ 开头的索引重命名，避免覆盖原有的数据。
* rename_replacement 表示重命名的格式为 restored_index_xxx ，如index_1会被重命名为restored_index_1 。

5. 查看恢复进度

```http
# 查看所有索引的恢复进度
curl -XGET http://192.168.0.1:9200/_recovery/
 
# 查看索引restored_index_1的恢复进度
curl -XGET http://192.168.0.1:9200/_recovery/restored_index_1
```

### 节点

节点的类型：

* 主节点：当一个节点 node.master = true 时，有资格被选为主节点，只有主节点拥有修改集群状态的能力，负责决定哪些分片分配给哪个节点，并在节点之间移动分片以平衡整个集群。
* 数据节点：当一个节点 node.data = ture 时，该节点不会被选择为主节点，负责保存数据和执行数据相关的操作，如crud。
* 客户端节点：当一个节点 node.master = false 和 node.data = false时，既不保存数据也不能成为主节点，但可以作为客户端节点，响应用户请求，并把相关操作发送到其他节点。
* 部落节点：当一个节点配置 tribe.* 时，可以连接多个集群，在所有连接的集群是哪个执行搜索和其他操作,例如：
  ```yaml
  tribe:
    tl:
      cluster.name: cluster_one
    t2:
      cluster.name: cluster_two
  ```

#### 主节点的选举

节点探索：启动节点时，会寻找拥有相同集群名称的主节点，如果找到则加入集群，如果没有找到则自己选为主节点。

主节点会检查其他索引节点是否能够响应，其他节点业务ping主节点，如果主节点断开，则选择新的主节点。

默认情况下每个节点都可以成为主节点和数据节点，也可以通过 node.master 和 node.data 配置。

为了避免脑裂，会设置集群最小互相连接的节点数目,`discovery.zen.minimum_master_nodes=主节点候选总数/2+1`。

### 分片

分片是数据的最小单元块，ES默认使用5个分片和1个副本。

分片的数量：

1. 如果有一个有限且明确的数据集，可以只使用一个分片。
2. 如果没有明确，最理想的分片数量应该依赖于节点数量，为了保证高可用和查询的吞吐量，需要配置副本数量。
3. 每个节点的分片数量保持在低于每1GB堆内存对应集群的分片在20-25之间。

分片的大小：通常大小在20GB和40GB之间。

#### 分片分配器

默认使用 balanced 分配器,自定义需实现 `org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocator`

> https://www.elastic.co/guide/en/elasticsearch/reference/8.6/modules-cluster.html#shards-rebalancing-heuristics

##### 分片决策者

分片分配器的分片逻辑由分片决策者处理，分配器包含多个决策者，可通过参数配置分片规则。

负载均衡类：

* SameShardAllocationDecider ：不允许相同分片（primary\replication）出现在相同的节点上，也考虑同一物理机多个es实例的情况（es可能多个虚拟机上，多个虚拟机在一台物理机上），通过`cluster.routing.allocation.same_shard.host`来配置，默认为false。
* ThrottlingAllocationDecider ：控制恢复过程中分片的并发数，可配置参数有 `cluster.routing.allocation.node_initial_primaries_recoveries` 和 `cluster.routing.allocation.node_concurrent_recoveries`，详情查看 集群再平衡 模块。
* AwarenessAllocationDecider ：更够感知服务器、服务机架等，尽量分散存储分片，参数举例:
  按机架分：`cluster.routing.allocation.awareness.attributes: rack_id`,`node.attr.rack_id:1`,`node.attr.rack_id:2`
  按区域分：`cluster.routing.allocation.awareness.attributes: zone`,`cluster.routing.allocation.awareness.force.zone.values: zone1,zone2`,如果zone1的机器上不能容纳所有的shard，并且zone2没有启动，剩余没有分配的shard则不会进行分配（zone1过载），直到等到zone2启动才进行分配。

并发数量类：

* ConcurrentRebalanceAllocationDecider : 再平衡并发数量，`cluster.routing.allocation.cluster_concurrent_rebalance`，详情查看 集群再平衡 模块。
* ThrottlingAllocationDecider : 恢复过程中并发数，`cluster.routing.allocation.node_initial_primaries_recoveries`，详情查看 集群再平衡 模块。

条件限制规则：

* FilterAllocationDecider ： 通过 include、exclude参数控制shard的节点分配，`index.routing.allocation.require.`、`index.routing.allocation.include.`、`index.routing.allocation.exclude.`、`cluster.routing.allocation.require.`、`cluster.routing.allocation.include.`、`cluster.routing.allocation.exclude.`，其中 require表示必须，include表示允许，exclude表示禁止，注意cluster的设置会重载掉index的配置。
* ReplicaAfterPrimaryActiveAllocationDecider ： 保证只会在主分片分配完毕后才开始分配分片副本。
* RebalanceOnlyWhenActiveAllocationDecider ：保证该索引的所有分片都在活跃状态才能进行rebalance过程。
* ClusterRebalanceAllocationDecider ：判断是否可以执行rebalance, `cluster.routing.allocation.allow_rebalance，详情查看 集群再平衡 模块。
* DiskThresholdDecider ：通过磁盘空间阈值来控制是否分配，默认关闭，通过`cluster.routing.allocation.disk.threshold_enabled`来开启，`cluster.routing.allocation.disk.watermark.low`允许用户指定百分比或者绝对数值来控制何时能够进行分片分配，默认值是0.7，表示当可用磁盘空间低于70%时，新的分片才可以分配到该节点上，`cluster.routing.allocation.disk.watermark.high`允许用户指定百分比或者绝对数值来控制何时需要将分片分配到其它的节点,默认值是0.85，表示当可用磁盘空间高于85%时，重新把该节点的分片分配到其它节点。

#### 主分片和复制分片的交互

每个节点都知道任意文档所在的节点，可以将请求转发到需要的节点，所以集群中任意节点都可以处理请求。

##### 写操作

新建、索引、删除都是写操作，它们必须在主分片上成功完成才能复制到相关的复制分片上。

1. 客户端发送请求到任意节点。
2. 节点使用_id确定文档分片，转发请求到具有该主分片的目标节点。
3. 目标节点在主分片上执行请求，如果成功，转发请求到主分片的的复制节点上，当所有复制节点响应成功，目标节点响应最初请求节点，请求节点再响应客户端。

replication：复制默认值为 sync，表示主分片需要等复制分片成功响应才返回，当设置为 async 时，主分片不会等复制分片响应，但依旧会转发请求。一般不使用，因为在不等待其他分片就绪就请求，可能导致请求过多而过载。

consistency：默认主分片在尝试写入时需要规定数量或过半的分片可用，计算方法为：( 1 个 primary + 复制分片的数量(不是现在活动的分片而是索引中的配置) )/2 + 1 。

新索引默认只有1个复制分片，表示需要2个活动分片，这个默认设置将阻止我们在单一节点集群中操作，可以将索引 number_of_replicas 设置大于1来改变。

timeout：当副本不足时，默认等待1分钟。

##### 检索文档

1. 客户端发送请求到任意节点。
2. 节点使用_id确定文档分片，然后请求具有该文档分片的节点，为了负责均衡，请求节点会为每个请求选择不同的分片即循环所有分片副本。
3. 目标节点返回数据给请求节点，然后请求节点返回给客户端。

##### 局部更新文档

1. 客户端发送请求到任意节点。
2. 转发请求到主分片所在节点。
3. 目标节点从主分片检索出文档，修改_source字段的字段，然后在主分片重建索引，如果有其他进程修改文档，以 retry_on_conflict 设置的次数重试，都为成功则放弃。
4. 如果 目标节点 更新文档成功，转发新文档到复制节点重建索引，当所有复制节点报告成功，目标节点返回成功到请求节点，然后请求节点返回给客户端。

##### 多文档操作

1. 客户端发送请求到任意节点。
2. 请求节点为每个分片构建一个多条数据请求，然后转发这些请求，当所有回复被接收，请求节点返回给客户端。

### 路由

当索引一个文档时，文档被存储在单独一个主分片上，路由分片的算法为 hash(routing) % number_of_primary_shards。

其中 routing 是一个任意字符串，默认值是 _id，也可以自定义，number_of_primary_shards 是主分片的数量。

例如：`curl -XPUT localhost:9200/documents/doc/1?routing=A,B -d '{"title":Document"}'`

#### 区域路由

可以通过在 elaticsearch.yml 配置 node.zone 来区分节点的区域，区域名称可随意命名。

例如：

1. 配置多个区域：

```yaml
# 节点1
node.zone: zone_one
# 节点2
node.zone: zone_two
```

2. 创建索引时指定区域

```shell
curl -XPOST 'localhost:9200/shop'
curl -XPUT 'localhost:9200/shop/_settings' -d '{
    "index.routing.allocation.include.zone" : "zone_one,zone_two"
}'
# 排除节点可使用 index.routing.allocation.exclude.zone
```

#### IP路由

```shell
curl -XPUT 'localhost:9200/shop/_settings' -d '{
    "index.routing.allocation.include.ip" :"10.1.2.10,10.1.2.11"
}'
```

## 问题处理

### 深度分页

#### search after

适用于实时分页查询，但是只能做下一页的场景，不能随机指定页数查询。

例如：查询客户名字带有`diane`，并且订单时间大于`2020-01-03`的订单信息，并且查询的结果按照时间升序。

1. 查询第一页：

```http
GET /kibana_sample_data_ecommerce/_search
{
  "size" : 2,
  "query": {
    "bool": {
      "must": [
        {"match": {
          "customer_first_name": "Diane"
        }}
      ],
      "filter": {
        "range": {
          "order_date": {
            "gte": "2020-01-03"
          }
        }
      }
    }
  }, 
  "sort": [
    {
      "order_date": "desc",
      "_id": "asc"

    }
  ]
}
```

排序中加入_id,是为了在 order_date 一样的情况下使用 _id 排序，在查询结果中会返回：

```text
"sort" : [
          1580597280000,
          "RZz1f28BdseAsPClqbyw"
        ]
```

2. 查询第二页，使用 search after 带上第一页返回的数据。

```http
GET /kibana_sample_data_ecommerce/_search
{
  "size" : 2,
  "query": {
    "bool": {
      "must": [
        {"match": {
          "customer_first_name": "Diane"
        }}
      ],
      "filter": {
        "range": {
          "order_date": {
            "gte": "2020-01-03"
          }
        }
      }
    }
  }, 
  "search_after": 
      [
          1580597280000,
          "RZz1f28BdseAsPClqbyw"
        ],
  "sort": [
    {
      "order_date": "desc",
      "_id": "asc"

    }
  ]
}
```

#### scroll api

基于快照，不能用于实时性高的业务场景。

1. 第一次查询，会生成当前查询结果的快照，后续查询过程中新的数据不会被查询到。

```http
POST /kibana_sample_data_ecommerce/_search?scroll=1m
{
    "size": 10,
    "query": {
        "match_all" : {
        }
    }
}
```

这里的scroll参数，描述的是快照保存在search context的时间。
上面的查询结果会返回一个 scroll_id ，用于后面的查询。

2. 第二次查询使用 scroll_id，并且不需要index信息和size信息。

```http
POST /_search/scroll
{
  "scroll" : "1m",
  "scroll_id" : "DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAA5AWNGpKbFNMZnVSc3loXzQwb0tJZHBtZw=="
}
```

scroll 快速的原因：

ES的检索分为查询（query）和获取（fetch）两个阶段，query阶段比较高效，只是查询满足条件的文档id汇总起来。
fetch阶段则基于每个分片的结果在coordinating节点上进行全局排序，然后最终计算出结果。

scroll查询的时候，在query阶段把符合条件的文档id保存在前面提到的search context里。
后面每次scroll分批取回只是根据scroll_id定位到游标的位置，然后抓取size大小的结果集即可。

### 集群再平衡

再平衡是集群在各个不同的节点之间移动分片的过程。当集群发生改变时，可能会发生再平衡，有时并不希望再平衡发生，可以通过`_cluster/setting`设置。

再平衡控制使用的场景：

1. 网络已经过度使用，不希望给网络造成太多辅导。
2. 再整个集群重启后降低I/O子系统的压力，并且希望同一时刻有较小的分片和副本被初始化。

`_cluster/setting`设置分为两种：暂时性和永久性。

```shell
# 暂时性：设置的熟悉持续到下一次重启
PUT /_cluster/settings
{
  "transient": {
     "key": "value"
  }
}
# 永久性：重启后仍然生效
PUT /_cluster/settings
{
  "persistent": {
     "key": "value"
  }
}
```

1. 控制再平衡开始的时间，`cluster.routing.allocation.allow_rebalance`，不能在运行时更改，需要在配置文件中设置。
   * always：只要需要就开始再平衡。
   * indices_primaries_active：当所有主分片初始化完成之后开始再平衡。
   * indices_all_active：默认值，当所有分片和副本完成初始化之后开始再平衡。
2. 再平衡期间控制在节点间同时移动的分片个数，`cluster.routing.allocation.cluster_concurrent_rebalance`，默认值为2，如果集群由多节点构成可适当增加该值。
3. 控制在单个节点上同时进行初始化的分片个数，`cluster.routing.allocation.node_concurrent_recoveries`，默认值为2，表示主分片向复制分片传输数据的个数，分片的恢复是IO密集型。
4. 控制在单个节点上同时进行初始化的主分片个数，`cluster,routing.allocation.node_initial_primaries_recoveries`，默认值为4，主分片的恢复使用的是本地磁盘的数据。

### 预热

预热查询和普通查询类似，储存在_warmer的特殊索引上,另外不要让集群加载过多的预热查询，因为最终可能会花太多的时间预热。

1. 创建预热查询，可以为索引设置多个预热查询，但每个预热查询必须有唯一的名字，

```http
curl -XPUT 'localhost:9200/索引名称/_warmer/预热查询名称' -d '{
    "query" :{
        "match all" :[}
    },
    "facets" : {
        "warming facet" : {
            "terms" : {
                "field" : "tags"
            }
        }
    }
}
```

2. 获取预热查询

```http
curl -XGET 'localhost:9200/索引名称/_warmer/预热查询名称'
```

### 热点线程

```http
curl 'localhost:9200/_nodes/hot_threads'
curl 'localhost:9200/_nodes/<node_id>/hot_threads'
```

支持的参数：

* threads：输出的线程数量，默认3。
* interval：检查的时间区间，默认500ms。
* type：检查的类型，默认cpu，除此之外还有cpu，block，wait。
* snapshots：函数调用堆栈数量，默认10。

另外响应的信息并非JSON数据，而是普通文本。

例如:

```text
#  响应的第一部分 包含节点的基本信息
{Data-(110.188)-1}{67A1DwgCR_eM5eFS-6MR1Q}{qTPWEpF-Q4GTZIlWr3qUqA}{10.6.110.188}{10.6.110.188:9301}{dil}
# 响应的第二部分 展示占用百分百以及线程名称
78.4% (391.7ms out of 500ms) cpu usage by thread 'elasticsearch[Data-(110.188)-1][search][T#38]'
# 表示 5/10 —— 表示拍摄的 5 个快照具有相同的堆栈跟踪信息。
5/10 snapshots sharing following 35 elements
```

线程名称：

* search ：用于搜索。
* recovery_stream ：用于恢复模块事件
* cache ：用于缓存事件
* merge ：用于段合并线程
* index ：用于数据索引（写入）线程 等等。
